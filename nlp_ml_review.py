# -*- coding: utf-8 -*-
"""CustomerReview.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CkkoqAm5fHlAMyySvdbETEvqkCJiiACP
"""


# Import Libraries for BERTopic and Pytorch

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as Data
import torch.nn.functional as F
from wordcloud import WordCloud, STOPWORDS
import re
import nltk
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
stop_wd = set(stopwords.words('english'))
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from collections import Counter
import spacy
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import MultinomialNB
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
from torch.utils.data import DataLoader, TensorDataset
import plotly.express as px
import pickle
import seaborn as sns
import matplotlib.pyplot as plt

plt.style.use('default')
plt.rcParams['axes.grid'] = True
plt.rcParams['axes.facecolor'] = 'white'


sns.set_style("whitegrid")

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/My Drive/Datasets
data = pd.read_csv('reviews_for_classification.csv')
data.head()

# Remove the rows with missing values - review body
data = data[data['review_body'].notna()]

# Check the data after removing the missing values
data.head()
data.info()
data.describe()

# Remove the rows with missing values - review head
data = data[data['review_head'].notna()]

# Check the data after removing the missing values
data.head()
data.info()
# Countries with the most reviews
data['country'].value_counts()

# Impute the missing values for country with US
data['country'] = data['country'].fillna('US')

# Check the data after imputing the missing values
data.info()
# We will not impute the missing values for name as it is not important for our analysis

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('omw-1.4')  # Optional, for lemmatization if needed

# Manually downloading the tokenizer models
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('punkt_tab')  # If explicitly needed (though uncommon)

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

def clean_text(text):
    """Cleans input text by removing URLs, HTML tags, special characters,
    and stopwords, then applies stemming."""
    if not isinstance(text, str):
        return ""

    text = text.lower()  # Lowercasing
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)  # Remove URLs
    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags
    text = re.sub(r'[^a-zA-Z\s]', ' ', text)  # Remove special characters & numbers
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces

    tokens = word_tokenize(text)  # Tokenization
    stop_words = set(stopwords.words("english"))
    stemmer = PorterStemmer()

    cleaned_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(cleaned_tokens)

# Apply text cleaning function
data["cleaned_review_body"] = data["review_body"].apply(clean_text)
data["cleaned_review_head"] = data["review_head"].apply(clean_text)

# Print cleaned data
print(data)

# Flatten the review body and cleaned review body
flattened_review_body = data['review_body'].apply(lambda x: ' '.join(x.split()))
flattened_cleaned_review_body = data['cleaned_review_body'].apply(lambda x: ' '.join(x.split()))

"""### Exploratory Data Analysis"""

# Visualize the length of the review body and cleaned review body
plt.figure(figsize=(10, 5))
plt.hist(data['review_body'].str.len(), bins=30, alpha=0.5, label='Review Body')
plt.hist(data['cleaned_review_body'].str.len(), bins=30, alpha=0.5, label='Cleaned Review Body')
plt.legend()
plt.xlim(0, 2500)
plt.show()

# Data Transformation: Convert 'stars' column to numeric type
data['stars'] = pd.to_numeric(data['stars'], errors='coerce')

# Data Aggregation: Count the occurrences of each star rating
star_counts = data['stars'].value_counts().sort_index().reset_index()
star_counts.columns = ['stars', 'count']

# Create a bar chart using Plotly Express
fig = px.bar(star_counts,
             x='stars',
             y='count',
             color='stars',
             color_continuous_scale=px.colors.sequential.Plasma,
             text='count',
             labels={'stars': 'Star Rating', 'count': 'Number of Reviews'},
             title='Distribution of Star Ratings for Celsius Network Reviews')

# Customize the chart for better presentation
fig.update_layout(
    xaxis_title='Star Rating',
    yaxis_title='Number of Reviews',
    xaxis=dict(tickmode='linear', tick0=1, dtick=1),  # Ensure x-axis ticks are integers
    title_x=0.5,  # Center the title
    font=dict(size=12),
    plot_bgcolor='rgba(0,0,0,0)',  # Transparent background
    paper_bgcolor='rgba(0,0,0,0)'   # Transparent background
)

fig.update_traces(
    texttemplate='<b>%{text}</b>',  # Format text on bars
    textposition='outside',
    marker_line_color='rgb(0,0,0)',  # Add a black outline to the bars
    marker_line_width=1.5,
    opacity=0.8
)

fig.show()

# Group by stars and concatenate review bodies
grouped_reviews = data.groupby('stars')['review_body'].apply(' '.join)

# Create word clouds for each rating
for stars, text in grouped_reviews.items():
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'{stars}-Star Reviews Word Cloud')
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Function to preprocess text
def preprocess(text):
    tokens = word_tokenize(text.lower())
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]
    return tokens

# Group by stars and preprocess review bodies
grouped_reviews = data.groupby('stars')['review_body'].apply(lambda x: ' '.join(x)).reset_index()
grouped_reviews['processed'] = grouped_reviews['review_body'].apply(preprocess)

# Function to get top 5 words and their counts
def get_top_words(words, n=5):
    word_counts = Counter(words)
    return word_counts.most_common(n)

# Create bar charts for each rating
for _, row in grouped_reviews.iterrows():
    stars = row['stars']
    top_words = get_top_words(row['processed'])

    words, counts = zip(*top_words)

    fig, ax = plt.subplots(figsize=(12, 6))

    # Create the bar plot
    bars = ax.bar(words, counts, color='#b3cde3')  # Light blue color for all bars

    # Highlight the top count in red
    max_count_index = counts.index(max(counts))
    bars[max_count_index].set_color('#c85a5a')  # Professional red color

    ax.set_title(f'Top 5 Words for {stars}-Star Reviews', fontsize=16, fontweight='bold')
    ax.set_xlabel('Words', fontsize=12)
    ax.set_xticklabels(words, rotation=0, fontsize=10)

    # Remove y-axis values
    ax.set_yticks([])

    # Add count labels on top of each bar
    for i, count in enumerate(counts):
        ax.text(i, count, str(count), ha='center', va='bottom', fontweight='bold')

    # Remove top and right spines
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    # Add a light grid
    ax.yaxis.grid(True, linestyle='--', alpha=0.7)

    plt.tight_layout()
    plt.show()

# Check distribution of 'star' column
print(data['stars'].value_counts())

"""#### Named Entity Recognition"""

# Load the spaCy model
nlp = spacy.load("en_core_web_sm")
# Function to extract named entities
def extract_entities(text):
    doc = nlp(text)
    return [ent.label_ for ent in doc.ents]

# Extract entities for each review
data['entities'] = data['review_body'].apply(extract_entities)

# Group by stars and aggregate entities
grouped_reviews = data.groupby('stars')['entities'].sum().reset_index()

# Function to get top 5 entities and their counts
def get_top_entities(entities, n=5):
    entity_counts = Counter(entities)
    return entity_counts.most_common(n)

# Create bar charts for each rating
for _, row in grouped_reviews.iterrows():
    stars = row['stars']
    top_entities = get_top_entities(row['entities'])

    entities, counts = zip(*top_entities)

    plt.figure(figsize=(10, 5))
    bars = plt.bar(entities, counts)
    plt.title(f'Top 5 Named Entities for {stars}-Star Reviews')
    plt.xlabel('Entities')
    plt.ylabel('Count')
    plt.xticks(rotation=45)

    # Highlight the top count
    max_count = max(counts)
    for bar in bars:
        if bar.get_height() == max_count:
            bar.set_color('red')

    for i, count in enumerate(counts):
        plt.text(i, count, str(count), ha='center', va='bottom')

    plt.tight_layout()
    plt.show()

"""# MODEL BUILDING"""

from sklearn.feature_extraction.text import TfidfVectorizer


# Convert stars to sentiment
sentiment_map = {1: "negative", 2: "negative", 3: "neutral", 4: "positive", 5: "positive"}
data["sentiment"] = data["stars"].map(sentiment_map)

# Split data
X_train, X_test, y_train, y_test = train_test_split(data["review_body"], data["sentiment"], test_size=0.2, random_state=42)

# Convert labels to numbers
label_encoder = LabelEncoder()
y_train_enc = label_encoder.fit_transform(y_train)
y_test_enc = label_encoder.transform(y_test)

# TF-Idata Vectorization
tfidata = TfidfVectorizer(max_features=10000, ngram_range=(1,2), stop_words='english')
X_train_tfidata = tfidata.fit_transform(X_train)
X_test_tfidata = tfidata.transform(X_test)

# Train Naïve Bayes
nb_model = MultinomialNB(alpha=0.5)
nb_model.fit(X_train_tfidata, y_train_enc)
y_pred_nb = nb_model.predict(X_test_tfidata)
print("Naïve Bayes Performance:\n", classification_report(y_test_enc, y_pred_nb))

# Train XGBoost
xgb_model = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, use_label_encoder=False, eval_metric='mlogloss')
xgb_model.fit(X_train_tfidata, y_train_enc)
y_pred_xgb = xgb_model.predict(X_test_tfidata)
print("XGBoost Performance:\n", classification_report(y_test_enc, y_pred_xgb))

# Deep Learning Model in PyTorch
class SentimentModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.3):
        super(SentimentModel, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return self.softmax(x)

# Convert TF-Idata to PyTorch tensors
X_train_tensor = torch.tensor(X_train_tfidata.toarray(), dtype=torch.float32)
X_test_tensor = torch.tensor(X_test_tfidata.toarray(), dtype=torch.float32)
y_train_tensor = torch.tensor(y_train_enc, dtype=torch.long)
y_test_tensor = torch.tensor(y_test_enc, dtype=torch.long)

# Create DataLoader
dataset = TensorDataset(X_train_tensor, y_train_tensor)
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)

# Define model
input_size = X_train_tfidata.shape[1]
hidden_size = 256
output_size = len(label_encoder.classes_)
model = SentimentModel(input_size, hidden_size, output_size, dropout_rate=0.3)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)

# Training loop
epochs = 10
for epoch in range(epochs):
    for inputs, labels in dataloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# Evaluate model
with torch.no_grad():
    y_pred_dl = model(X_test_tensor).argmax(dim=1)
print("Deep Learning Model Performance:\n", classification_report(y_test_enc, y_pred_dl.numpy()))

import pandas as pd
from sklearn.metrics import classification_report, accuracy_score

# Function to extract metrics from classification report
def get_metrics(y_true, y_pred, model_name):
    report = classification_report(y_true, y_pred, output_dict=True)
    accuracy = accuracy_score(y_true, y_pred)

    return {
        "Model": model_name,
        "Accuracy": accuracy,
        "Precision (Macro Avg)": report["macro avg"]["precision"],
        "Recall (Macro Avg)": report["macro avg"]["recall"],
        "F1-Score (Macro Avg)": report["macro avg"]["f1-score"]
    }

# Get metrics for each model
nb_metrics = get_metrics(y_test_enc, y_pred_nb, "Naïve Bayes")
xgb_metrics = get_metrics(y_test_enc, y_pred_xgb, "XGBoost")
dl_metrics = get_metrics(y_test_enc, y_pred_dl.numpy(), "Deep Learning")

# Create DataFrame
metrics_data = pd.DataFrame([nb_metrics, xgb_metrics, dl_metrics])

# Display table
print(metrics_data)

import seaborn as sns
import matplotlib.pyplot as plt

# Set Seaborn style
sns.set_style("whitegrid")

# Create figure and axis
plt.figure(figsize=(8, 4))
ax = plt.gca()

# Create a heatmap table
sns.heatmap(metrics_data.set_index("Model"), annot=True, fmt=".2f", cmap="Blues", linewidths=0.5, cbar=False, ax=ax)

# Title and formatting
plt.title("Model Performance Metrics", fontsize=14)
plt.xticks(rotation=0)
plt.yticks(rotation=0)

# Show the table
plt.show()

"""## Time Series Analysis"""

import re

# Remove "Updated" and extra text
data["date_time"] = data["date_time"].apply(lambda x: re.sub(r"Updated ", "", str(x)))

# Convert to datetime (handle errors)
data["date_time"] = pd.to_datetime(data["date_time"], errors="coerce")

# Drop rows with invalid dates
data = data.dropna(subset=["date_time"])

import pandas as pd
import matplotlib.pyplot as plt
from prophet import Prophet

# Convert 'date_time' to datetime format
data["date_time"] = pd.to_datetime(data["date_time"])

# Aggregate sentiment trends (group by month)
data["month"] = data["date_time"].dt.to_period("M")  # Monthly aggregation
sentiment_trend = data.groupby(["month", "sentiment"]).size().unstack().fillna(0)

# Convert Period to DateTime format for Prophet
sentiment_trend.index = sentiment_trend.index.to_timestamp()

# Prepare Prophet dataframe (for negative sentiment)
data_prophet = sentiment_trend.reset_index()[["month", "negative"]]
data_prophet.columns = ["ds", "y"]  # Rename for Prophet

# Plot sentiment trend
plt.figure(figsize=(12, 6))
plt.plot(data_prophet["ds"], data_prophet["y"], marker="o", label="Negative Sentiment")
plt.xlabel("Time (Monthly)")
plt.ylabel("Count of Negative Reviews")
plt.title("Negative Sentiment Trend")
plt.legend()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from prophet import Prophet
import re

# Clean 'date_time' column
data["date_time"] = data["date_time"].apply(lambda x: re.sub(r"Updated ", "", str(x)))
data["date_time"] = pd.to_datetime(data["date_time"], errors="coerce")
data = data.dropna(subset=["date_time"])  # Drop invalid dates

# Aggregate sentiment trends (monthly)
data["month"] = data["date_time"].dt.to_period("M")
sentiment_trend = data.groupby(["month", "sentiment"]).size().unstack().fillna(0)

# Convert Period to DateTime format
sentiment_trend.index = sentiment_trend.index.to_timestamp()

# Prepare Prophet dataframe (for positive sentiment)
data_prophet = sentiment_trend.reset_index()[["month", "positive"]]
data_prophet.columns = ["ds", "y"]  # Rename for Prophet

# Plot positive sentiment trend
plt.figure(figsize=(12, 6))
plt.plot(data_prophet["ds"], data_prophet["y"], marker="o", label="Positive Sentiment", color="green")
plt.xlabel("Time (Monthly)")
plt.ylabel("Count of Positive Reviews")
plt.title("Positive Sentiment Trend")
plt.legend()
plt.show()



import pandas as pd
import re
import numpy as np
import plotly.graph_objects as go
from prophet import Prophet
from dash import Dash, dcc, html, Input, Output
from sklearn.metrics import mean_squared_error


# Convert stars to sentiment
sentiment_map = {1: "negative", 2: "negative", 3: "neutral", 4: "positive", 5: "positive"}
data["sentiment"] = data["stars"].map(sentiment_map)

# Aggregate Monthly Sentiment Trends
data["month"] = data["date_time"].dt.to_period("M")
sentiment_trend = data.groupby(["month", "sentiment"]).size().unstack().fillna(0)
sentiment_trend.index = sentiment_trend.index.to_timestamp()

# Prepare DataFrames for Prophet
def prepare_prophet_data(sentiment):
    data_prophet = sentiment_trend.reset_index()[["month", sentiment]]
    data_prophet.columns = ["ds", "y"]
    return data_prophet

data_positive = prepare_prophet_data("positive")
data_negative = prepare_prophet_data("negative")

# Train Prophet Model for Both Sentiments
def train_forecast_prophet(data):
    train_size = int(len(data) * 0.8)  # 80% training, 20% testing
    train_data, test_data = data[:train_size], data[train_size:]

    # Train Prophet
    model = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)
    model.fit(train_data)

    # Forecast Future Dates
    future = model.make_future_dataframe(periods=12, freq="M")
    forecast = model.predict(future)

    # Match test and forecast data length
    test_pred = forecast.iloc[train_size:train_size + len(test_data)]  # Ensure same length
    test_data = test_data.iloc[:len(test_pred)]  # Match test set length

    # Validate with RMSE
    rmse = np.sqrt(mean_squared_error(test_data["y"], test_pred["yhat"]))

    return model, forecast, rmse

model_positive, forecast_positive, rmse_positive = train_forecast_prophet(data_positive)
model_negative, forecast_negative, rmse_negative = train_forecast_prophet(data_negative)

# Create Interactive Dashboard
app = Dash(__name__)

app.layout = html.Div([
    html.H1("Sentiment Trend & Forecasting", style={"textAlign": "center"}),

    # Dropdown to Select Sentiment
    dcc.Dropdown(
        id="sentiment-dropdown",
        options=[
            {"label": "Positive Sentiment", "value": "positive"},
            {"label": "Negative Sentiment", "value": "negative"}
        ],
        value="positive",
        clearable=False,
        style={"width": "50%", "margin": "auto"}
    ),

    # Time Series Graph
    dcc.Graph(id="sentiment-trend-graph"),

    # Forecast Graph
    dcc.Graph(id="forecast-graph"),

    # Display RMSE Score
    html.Div(id="rmse-score", style={"textAlign": "center", "fontSize": "20px", "marginTop": "20px"})
])

@app.callback(
    [Output("sentiment-trend-graph", "figure"),
     Output("forecast-graph", "figure"),
     Output("rmse-score", "children")],
    [Input("sentiment-dropdown", "value")]
)
def update_graph(selected_sentiment):
    if selected_sentiment == "positive":
        data = data_positive
        forecast = forecast_positive
        rmse = rmse_positive
        color = "green"
    else:
        data = data_negative
        forecast = forecast_negative
        rmse = rmse_negative
        color = "red"

    # Time Series Graph
    fig_trend = go.Figure()
    fig_trend.add_trace(go.Scatter(x=data["ds"], y=data["y"], mode="lines+markers", name=f"{selected_sentiment} reviews", line=dict(color=color)))
    fig_trend.update_layout(title=f"{selected_sentiment.capitalize()} Sentiment Trend Over Time", xaxis_title="Time", yaxis_title="Count", template="plotly_dark")

    # Forecast Graph
    fig_forecast = go.Figure()
    fig_forecast.add_trace(go.Scatter(x=forecast["ds"], y=forecast["yhat"], mode="lines", name="Forecast", line=dict(color="blue")))
    fig_forecast.add_trace(go.Scatter(x=data["ds"], y=data["y"], mode="lines+markers", name="Actual", line=dict(color=color)))
    fig_forecast.update_layout(title=f"12-Month Forecast for {selected_sentiment.capitalize()} Sentiment", xaxis_title="Time", yaxis_title="Count", template="plotly_dark")

    # RMSE Score
    rmse_text = f"Forecast RMSE for {selected_sentiment.capitalize()} Sentiment: {rmse:.2f}"

    return fig_trend, fig_forecast, rmse_text

if __name__ == "__main__":
    app.run_server(debug=True)



"""## Topic Modeling using BERTopic & Anomaly Review Detection"""

import pandas as pd
import numpy as np
import re
from bertopic import BERTopic
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, HTML

# Download necessary NLTK data (only needs to be done once)
try:
    stopwords.words('english')
except LookupError:
    nltk.download('stopwords')
    nltk.download('punkt')

# 1. Data Preparation

# Data Transformation: Convert 'stars' column to numeric type
data['stars'] = pd.to_numeric(data['stars'], errors='coerce')

data['cleaned_review'] = data['review_body'].apply(clean_text)

# Remove Stop Words
stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    word_tokens = word_tokenize(text)
    filtered_sentence = [w for w in word_tokens if not w in stop_words]
    return ' '.join(filtered_sentence)

data['cleaned_review'] = data['cleaned_review'].apply(remove_stopwords)

# 2. BERTopic for Topic Modeling

# Load a pre-trained BERTopic model (or train your own)
topic_model = BERTopic(language="english", calculate_probabilities=True, verbose=False) # Disable verbose output
topics, probabilities = topic_model.fit_transform(data['cleaned_review'])

# Add topic information to the DataFrame
data['topic'] = topics

# Get topic distribution for each document
topic_probs_data = pd.DataFrame(probabilities)

# 3. Anomaly Detection on Topic Distributions

# Scale the topic probabilities
scaler = StandardScaler()
scaled_topic_probs = scaler.fit_transform(topic_probs_data)

# Train Isolation Forest
iso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)
iso_forest.fit(scaled_topic_probs)

# Get anomaly scores
anomaly_scores = iso_forest.decision_function(scaled_topic_probs)
data['anomaly_score'] = anomaly_scores

# Label anomalies
threshold = np.percentile(anomaly_scores, 5) # 5th percentile
data['is_anomaly'] = data['anomaly_score'] <= threshold

# 4. Analyze Anomalies and Prepare Data for Table

# Get Topic Info for all topics
topic_info = topic_model.get_topic_info()

# Get some example anomalous review
example_anomalous_reviews = data[data['is_anomaly'] == True]['review_body'].head(5).tolist()
example_anomalous_reviews_str = "<br><br>".join([f"<li>{review}</li>" for review in example_anomalous_reviews])

# Prepare topic legend data
top_k = 30
topic_counts = data['topic'].value_counts()
top_topics = topic_counts.nlargest(top_k).index
topic_legend_data = []
for topic in top_topics:
    topic_name = topic_info[topic_info['Topic'] == topic]['Name'].values[0]
    topic_legend_data.append({
        "Topic": topic,
        "Topic Name": topic_name
    })
topic_legend_data = pd.DataFrame(topic_legend_data)

# Prepare top topic info data
top_topic_info = topic_info[topic_info['Topic'].isin(top_topics)].sort_values(by='Topic')
top_topic_info = top_topic_info[['Topic', 'Name', 'Count']].reset_index(drop=True)

# Create HTML table for top topic legend
topic_legend_html = topic_legend_data.to_html(classes='table table-striped', escape=False, index=False)

# Create HTML table for top topic info
top_topic_info_html = top_topic_info.to_html(classes='table table-striped', escape=False, index=False)

# Create a summary table
summary_data = []
for i in range(min(10, len(data))):  # Display anomaly score for the first 10 reviews, max
    summary_data.append({
        "Review": data['review_body'].iloc[i][:100] + "...",  # Truncate review for display
        "Anomaly Score": data['anomaly_score'].iloc[i],
        "Is Anomaly": data['is_anomaly'].iloc[i]
    })

summary_table_data = pd.DataFrame(summary_data)

# Create HTML table for anomaly scores
anomaly_table_html = summary_table_data.to_html(classes='table table-striped', escape=False, index=False)

# Combine all tables and example reviews into a single HTML block
html_output = f"""
<style>
    .table {{
        width: 100%;
        border-collapse: collapse;
        margin-bottom: 15px;
    }}
    .table th, .table td {{
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;
    }}
    .table th {{
        background-color: #f2f2f2;
        font-weight: bold;
    }}
</style>

<h2>Topic Legend</h2>
{topic_legend_html}

<h2>Top Topic Information</h2>
{top_topic_info_html}

<h2>Example Anomalous Reviews</h2>
<ul>
    {example_anomalous_reviews_str}
</ul>

<h2>Anomaly Scores for First 10 Reviews</h2>
{anomaly_table_html}
"""

# Display the HTML
display(HTML(html_output))

# a. Topic Distribution by Star Rating (Bar Chart with Hover Data)
topic_star_counts = data.groupby(['topic', 'stars']).size().reset_index(name='count')

# Filter to keep only top topics
topic_star_counts = topic_star_counts[topic_star_counts['topic'].isin(top_topics)]

# Add topic name to the topic_star_counts DataFrame
topic_names = {topic: topic_info[topic_info['Topic'] == topic]['Name'].values[0] for topic in topic_star_counts['topic'].unique()}
topic_star_counts['topic_name'] = topic_star_counts['topic'].map(topic_names)

fig = px.bar(topic_star_counts, x='topic', y='count', color='stars',
             title='Topic Distribution by Star Rating (Top 30 Topics)',
             labels={'topic': 'Topic', 'count': 'Number of Reviews', 'stars': 'Star Rating'},
             hover_data=['topic_name']) # Add topic name to hover data
fig.show()

# b. Average Star Rating per Topic (Bar Chart) - TOP K
data_top_topics = data[data['topic'].isin(top_topics)] # Filter the dataframe to include only those topics

avg_stars_per_topic = data_top_topics.groupby('topic')['stars'].mean().sort_values(ascending=False) # Calculate average star rating per topic
fig, ax = plt.subplots(figsize=(16, 8)) # Increase figure size for better readability
avg_stars_per_topic.plot(kind='bar', ax=ax, color='skyblue')

# Set x-axis labels to topic numbers, not descriptions
ax.set_xticks(range(len(avg_stars_per_topic)))
ax.set_xticklabels(avg_stars_per_topic.index, rotation=45, ha="right")  # Rotate labels for readability

ax.set_title(f'Average Star Rating per Top {top_k} Topics')
ax.set_xlabel('Topic')
ax.set_ylabel('Average Star Rating')
plt.tight_layout()
plt.show()

# c. Anomaly Distribution by Star Rating (Box Plot)
plt.figure(figsize=(10, 6))
sns.boxplot(x='stars', y='anomaly_score', data=data)
plt.title('Anomaly Score Distribution by Star Rating')
plt.xlabel('Star Rating')
plt.ylabel('Anomaly Score')
plt.show()

# d. Topic Word Cloud
topic_model.visualize_topics()

# e. Intertopic Distance Map
topic_model.visualize_heatmap()

# f. Topic Hierarchy
topic_model.visualize_hierarchy()

# Optional: Evaluate if you have ground truth labels
if 'is_fake' in data.columns:
    from sklearn.metrics import classification_report, roc_auc_score, roc_curve

    print(classification_report(data['is_fake'], data['is_anomaly']))
    auc = roc_auc_score(data['is_fake'], data['anomaly_score'])
    print(f"AUC: {auc}")

    # Plot ROC curve
    fpr, tpr, thresholds = roc_curve(data['is_fake'], data['anomaly_score'])
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend()
    plt.show()
else:
    print("No ground truth 'is_fake' column available for evaluation.")

"""# Final ML Model For Production"""

# Train XGBoost
xgb_model = XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, use_label_encoder=False, eval_metric='mlogloss')
xgb_model.fit(X_train_tfidf, y_train_enc)
y_pred_xgb = xgb_model.predict(X_test_tfidf)
print("XGBoost Performance:\n", classification_report(y_test_enc, y_pred_xgb))

# Save the XGBoost model
with open('xgboost_sentiment_model.pkl', 'wb') as model_file:
    pickle.dump(xgb_model, model_file)

print("XGBoost model saved successfully.")

"""Load Model"""

# # Load the XGBoost model
# with open('xgboost_sentiment_model.pkl', 'rb') as model_file:
#     loaded_xgb_model = pickle.load(model_file)

# # Use the loaded model for predictions
# new_data_tfidf = tfidf.transform(new_data)  # Assuming 'tfidf' is your fitted TfidfVectorizer
# predictions = loaded_xgb_model.predict(new_data_tfidf)

# Save TfidfVectorizer
with open('tfidf_vectorizer.pkl', 'wb') as tfidf_file:
    pickle.dump(tfidf, tfidf_file)

# Save LabelEncoder
with open('label_encoder.pkl', 'wb') as le_file:
    pickle.dump(label_encoder, le_file)

